{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3deae552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ae7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"BostonHousing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d03e472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e246ef19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim          zn       indus        chas         nox          rm  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              age         dis         rad         tax     ptratio           b  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            lstat        medv  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ff165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crim       0\n",
       "zn         0\n",
       "indus      0\n",
       "chas       0\n",
       "nox        0\n",
       "rm         0\n",
       "age        0\n",
       "dis        0\n",
       "rad        0\n",
       "tax        0\n",
       "ptratio    0\n",
       "b          0\n",
       "lstat      0\n",
       "medv       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aafa975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n",
       "       'ptratio', 'b', 'lstat', 'medv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a011ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.388305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.360445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.483725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.175260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.427321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.695360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.376955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>-0.381626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>-0.468536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.507787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>0.333461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.737663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medv</th>\n",
       "      <td>-0.388305</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim        zn     indus      chas       nox        rm       age  \\\n",
       "crim     1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
       "zn      -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "indus    0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "chas    -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "nox      0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "rm      -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "age      0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "dis     -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "rad      0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "tax      0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "ptratio  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "b       -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "lstat    0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "medv    -0.388305  0.360445 -0.483725  0.175260 -0.427321  0.695360 -0.376955   \n",
       "\n",
       "              dis       rad       tax   ptratio         b     lstat      medv  \n",
       "crim    -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621 -0.388305  \n",
       "zn       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  0.360445  \n",
       "indus   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800 -0.483725  \n",
       "chas    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  0.175260  \n",
       "nox     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879 -0.427321  \n",
       "rm       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  0.695360  \n",
       "age     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339 -0.376955  \n",
       "dis      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  0.249929  \n",
       "rad     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676 -0.381626  \n",
       "tax     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993 -0.468536  \n",
       "ptratio -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044 -0.507787  \n",
       "b        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  0.333461  \n",
       "lstat   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000 -0.737663  \n",
       "medv     0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e747e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.iloc[:,[5,12]]\n",
    "Y= df.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2bb3218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rm</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.575</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.421</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.185</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.998</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.147</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rm  lstat\n",
       "0  6.575   4.98\n",
       "1  6.421   9.14\n",
       "2  7.185   4.03\n",
       "3  6.998   2.94\n",
       "4  7.147   5.33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f05bddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2\n",
       "Name: medv, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da103110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c45da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b64087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model=Sequential()\n",
    "model.add(keras.Input(shape=(2,)))\n",
    "model.add(keras.layers.Dense(30))\n",
    "model.add(keras.layers.Dense(20))\n",
    "model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9417e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                90        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                620       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 731\n",
      "Trainable params: 731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e15de013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method compile in module keras.engine.training:\n",
      "\n",
      "compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, jit_compile=None, **kwargs) method of keras.engine.sequential.Sequential instance\n",
      "    Configures the model for training.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
      "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
      "                  metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
      "                           tf.keras.metrics.FalseNegatives()])\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "        optimizer: String (name of optimizer) or optimizer instance. See\n",
      "          `tf.keras.optimizers`.\n",
      "        loss: Loss function. May be a string (name of loss function), or\n",
      "          a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
      "          function is any callable with the signature `loss = fn(y_true,\n",
      "          y_pred)`, where `y_true` are the ground truth values, and\n",
      "          `y_pred` are the model's predictions.\n",
      "          `y_true` should have shape\n",
      "          `(batch_size, d0, .. dN)` (except in the case of\n",
      "          sparse loss functions such as\n",
      "          sparse categorical crossentropy which expects integer arrays of\n",
      "          shape `(batch_size, d0, .. dN-1)`).\n",
      "          `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
      "          The loss function should return a float tensor.\n",
      "          If a custom `Loss` instance is\n",
      "          used and reduction is set to `None`, return value has shape\n",
      "          `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
      "          values; otherwise, it is a scalar. If the model has multiple\n",
      "          outputs, you can use a different loss on each output by passing a\n",
      "          dictionary or a list of losses. The loss value that will be\n",
      "          minimized by the model will then be the sum of all individual\n",
      "          losses, unless `loss_weights` is specified.\n",
      "        metrics: List of metrics to be evaluated by the model during\n",
      "          training and testing. Each of this can be a string (name of a\n",
      "          built-in function), function or a `tf.keras.metrics.Metric`\n",
      "          instance. See `tf.keras.metrics`. Typically you will use\n",
      "          `metrics=['accuracy']`.\n",
      "          A function is any callable with the signature `result = fn(y_true,\n",
      "          y_pred)`. To specify different metrics for different outputs of a\n",
      "          multi-output model, you could also pass a dictionary, such as\n",
      "          `metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']}`.\n",
      "          You can also pass a list to specify a metric or a list of metrics\n",
      "          for each output, such as\n",
      "          `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
      "          or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
      "          strings 'accuracy' or 'acc', we convert this to one of\n",
      "          `tf.keras.metrics.BinaryAccuracy`,\n",
      "          `tf.keras.metrics.CategoricalAccuracy`,\n",
      "          `tf.keras.metrics.SparseCategoricalAccuracy` based on the shapes\n",
      "          of the targets and of the model output. We do a similar\n",
      "          conversion for the strings 'crossentropy' and 'ce' as well.\n",
      "          The metrics passed here are evaluated without sample weighting; if\n",
      "          you would like sample weighting to apply, you can specify your\n",
      "          metrics via the `weighted_metrics` argument instead.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "          coefficients (Python floats) to weight the loss contributions of\n",
      "          different model outputs. The loss value that will be minimized by\n",
      "          the model will then be the *weighted sum* of all individual\n",
      "          losses, weighted by the `loss_weights` coefficients.  If a list,\n",
      "          it is expected to have a 1:1 mapping to the model's outputs. If a\n",
      "          dict, it is expected to map output names (strings) to scalar\n",
      "          coefficients.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted by\n",
      "          `sample_weight` or `class_weight` during training and testing.\n",
      "        run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
      "          logic will not be wrapped in a `tf.function`. Recommended to leave\n",
      "          this as `None` unless your `Model` cannot be run inside a\n",
      "          `tf.function`. `run_eagerly=True` is not supported when using\n",
      "          `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "        steps_per_execution: Int. Defaults to 1. The number of batches to\n",
      "          run during each `tf.function` call. Running multiple batches\n",
      "          inside a single `tf.function` call can greatly improve performance\n",
      "          on TPUs or small models with a large Python overhead. At most, one\n",
      "          full epoch will be run each execution. If a number larger than the\n",
      "          size of the epoch is passed, the execution will be truncated to\n",
      "          the size of the epoch. Note that if `steps_per_execution` is set\n",
      "          to `N`, `Callback.on_batch_begin` and `Callback.on_batch_end`\n",
      "          methods will only be called every `N` batches (i.e. before/after\n",
      "          each `tf.function` execution).\n",
      "        jit_compile: If `True`, compile the model training step with XLA.\n",
      "          [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n",
      "          for machine learning.\n",
      "          `jit_compile` is not enabled for by default.\n",
      "          Note that `jit_compile=True`\n",
      "          may not necessarily work for all models.\n",
      "          For more information on supported operations please refer to the\n",
      "          [XLA documentation](https://www.tensorflow.org/xla).\n",
      "          Also refer to\n",
      "          [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n",
      "          for more details.\n",
      "        **kwargs: Arguments supported for backwards compatibility only.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c72c2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\",loss=\"mse\",metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3398c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 2s 39ms/step - loss: 592.8424 - mae: 22.7152 - val_loss: 515.2867 - val_mae: 21.3165\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 577.3092 - mae: 22.5447 - val_loss: 500.2039 - val_mae: 21.1196\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 559.2178 - mae: 22.3208 - val_loss: 483.7163 - val_mae: 20.8660\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 539.5983 - mae: 22.0404 - val_loss: 463.2272 - val_mae: 20.5177\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 513.8564 - mae: 21.6233 - val_loss: 439.6148 - val_mae: 20.0576\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 485.6616 - mae: 21.1178 - val_loss: 411.0457 - val_mae: 19.4419\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 451.0418 - mae: 20.4137 - val_loss: 379.3438 - val_mae: 18.6625\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 413.0813 - mae: 19.5413 - val_loss: 344.0455 - val_mae: 17.6763\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 372.6677 - mae: 18.4876 - val_loss: 306.4433 - val_mae: 16.4760\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 327.7460 - mae: 17.2205 - val_loss: 266.7844 - val_mae: 15.1067\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 281.7277 - mae: 15.7621 - val_loss: 224.1218 - val_mae: 13.5590\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 233.6224 - mae: 14.0987 - val_loss: 180.1531 - val_mae: 11.8059\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 184.9596 - mae: 12.2678 - val_loss: 136.4678 - val_mae: 9.9127\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 138.4883 - mae: 10.2489 - val_loss: 97.7096 - val_mae: 7.8609\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 97.7526 - mae: 8.2083 - val_loss: 67.1618 - val_mae: 5.9820\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 67.3585 - mae: 6.3345 - val_loss: 46.2131 - val_mae: 4.4333\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 47.0484 - mae: 4.8691 - val_loss: 36.1516 - val_mae: 3.8375\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 36.5290 - mae: 4.1720 - val_loss: 32.2304 - val_mae: 3.8994\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 32.1700 - mae: 3.9395 - val_loss: 32.3156 - val_mae: 4.1131\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.7427 - mae: 3.8908 - val_loss: 32.9257 - val_mae: 4.2434\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.4596 - mae: 3.9208 - val_loss: 33.7667 - val_mae: 4.3633\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4765 - mae: 3.9527 - val_loss: 34.1801 - val_mae: 4.4228\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4785 - mae: 3.9556 - val_loss: 33.9797 - val_mae: 4.3940\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4516 - mae: 3.9474 - val_loss: 33.5402 - val_mae: 4.3454\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4777 - mae: 3.9451 - val_loss: 33.2478 - val_mae: 4.3169\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 30.4381 - mae: 3.9413 - val_loss: 33.3188 - val_mae: 4.3296\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.6037 - mae: 3.9498 - val_loss: 32.9636 - val_mae: 4.2991\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4356 - mae: 3.9416 - val_loss: 33.5604 - val_mae: 4.3519\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.6434 - mae: 3.9398 - val_loss: 33.8353 - val_mae: 4.3636\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.9020 - mae: 3.9642 - val_loss: 32.8228 - val_mae: 4.2932\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.6934 - mae: 3.9603 - val_loss: 33.8962 - val_mae: 4.3885\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.5907 - mae: 3.9629 - val_loss: 33.8179 - val_mae: 4.3841\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4727 - mae: 3.9488 - val_loss: 32.9943 - val_mae: 4.2982\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4781 - mae: 3.9430 - val_loss: 33.3610 - val_mae: 4.3295\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4933 - mae: 3.9432 - val_loss: 33.8004 - val_mae: 4.3734\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 30.5326 - mae: 3.9480 - val_loss: 33.3374 - val_mae: 4.3350\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4662 - mae: 3.9484 - val_loss: 33.4255 - val_mae: 4.3405\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.6733 - mae: 3.9413 - val_loss: 34.1085 - val_mae: 4.3939\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5346 - mae: 3.9365 - val_loss: 33.0165 - val_mae: 4.2883\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.5810 - mae: 3.9381 - val_loss: 33.0233 - val_mae: 4.2960\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.6414 - mae: 3.9578 - val_loss: 33.9321 - val_mae: 4.3985\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.4868 - mae: 3.9629 - val_loss: 33.6389 - val_mae: 4.3757\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4290 - mae: 3.9533 - val_loss: 33.5201 - val_mae: 4.3503\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4575 - mae: 3.9399 - val_loss: 33.3519 - val_mae: 4.3250\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4516 - mae: 3.9387 - val_loss: 33.2075 - val_mae: 4.3100\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5915 - mae: 3.9346 - val_loss: 32.8013 - val_mae: 4.2742\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5929 - mae: 3.9434 - val_loss: 33.8353 - val_mae: 4.3636\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.4703 - mae: 3.9325 - val_loss: 33.4522 - val_mae: 4.3276\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4965 - mae: 3.9371 - val_loss: 33.3031 - val_mae: 4.3233\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 30.5290 - mae: 3.9531 - val_loss: 33.8217 - val_mae: 4.3843\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4578 - mae: 3.9554 - val_loss: 33.5703 - val_mae: 4.3577\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.6431 - mae: 3.9486 - val_loss: 33.9294 - val_mae: 4.3740\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.6781 - mae: 3.9477 - val_loss: 32.8628 - val_mae: 4.2826\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5382 - mae: 3.9390 - val_loss: 33.6661 - val_mae: 4.3531\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4763 - mae: 3.9344 - val_loss: 33.6218 - val_mae: 4.3410\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.5814 - mae: 3.9426 - val_loss: 33.1438 - val_mae: 4.3074\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.7836 - mae: 3.9479 - val_loss: 34.1092 - val_mae: 4.3890\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4818 - mae: 3.9521 - val_loss: 33.2437 - val_mae: 4.3361\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4586 - mae: 3.9548 - val_loss: 33.4095 - val_mae: 4.3413\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4847 - mae: 3.9488 - val_loss: 33.2778 - val_mae: 4.3315\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 30.6532 - mae: 3.9535 - val_loss: 34.1573 - val_mae: 4.4113\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4419 - mae: 3.9455 - val_loss: 33.4769 - val_mae: 4.3405\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4888 - mae: 3.9489 - val_loss: 33.1797 - val_mae: 4.3216\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4772 - mae: 3.9514 - val_loss: 33.6910 - val_mae: 4.3634\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4987 - mae: 3.9414 - val_loss: 33.6355 - val_mae: 4.3487\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.6001 - mae: 3.9405 - val_loss: 33.3117 - val_mae: 4.3214\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.5347 - mae: 3.9583 - val_loss: 33.9856 - val_mae: 4.4013\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.6027 - mae: 3.9481 - val_loss: 33.4108 - val_mae: 4.3192\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5199 - mae: 3.9384 - val_loss: 33.5923 - val_mae: 4.3364\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4456 - mae: 3.9293 - val_loss: 33.4132 - val_mae: 4.3266\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.4456 - mae: 3.9438 - val_loss: 33.7711 - val_mae: 4.3775\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.5469 - mae: 3.9609 - val_loss: 33.6934 - val_mae: 4.3730\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4799 - mae: 3.9523 - val_loss: 33.4162 - val_mae: 4.3329\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.5722 - mae: 3.9493 - val_loss: 33.3010 - val_mae: 4.3301\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4325 - mae: 3.9476 - val_loss: 33.4244 - val_mae: 4.3364\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.6693 - mae: 3.9545 - val_loss: 33.3902 - val_mae: 4.3393\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.7422 - mae: 3.9601 - val_loss: 35.0478 - val_mae: 4.4816\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.7273 - mae: 3.9450 - val_loss: 33.2087 - val_mae: 4.3016\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4954 - mae: 3.9371 - val_loss: 33.5287 - val_mae: 4.3424\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5555 - mae: 3.9458 - val_loss: 33.6018 - val_mae: 4.3459\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4810 - mae: 3.9416 - val_loss: 33.8207 - val_mae: 4.3740\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4639 - mae: 3.9423 - val_loss: 33.9071 - val_mae: 4.3792\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4749 - mae: 3.9406 - val_loss: 33.9543 - val_mae: 4.3749\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4604 - mae: 3.9408 - val_loss: 33.7215 - val_mae: 4.3465\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4802 - mae: 3.9367 - val_loss: 33.3623 - val_mae: 4.3229\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.5094 - mae: 3.9457 - val_loss: 33.0909 - val_mae: 4.3074\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.5036 - mae: 3.9410 - val_loss: 33.7403 - val_mae: 4.3575\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4179 - mae: 3.9363 - val_loss: 33.5082 - val_mae: 4.3513\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4842 - mae: 3.9578 - val_loss: 33.2225 - val_mae: 4.3311\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5113 - mae: 3.9610 - val_loss: 33.4823 - val_mae: 4.3538\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.5648 - mae: 3.9586 - val_loss: 33.7214 - val_mae: 4.3798\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.4847 - mae: 3.9509 - val_loss: 33.9929 - val_mae: 4.3929\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5803 - mae: 3.9524 - val_loss: 33.0918 - val_mae: 4.3084\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.4767 - mae: 3.9489 - val_loss: 33.2676 - val_mae: 4.3230\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5035 - mae: 3.9513 - val_loss: 33.7530 - val_mae: 4.3671\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5967 - mae: 3.9305 - val_loss: 33.6488 - val_mae: 4.3300\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.5368 - mae: 3.9291 - val_loss: 33.7574 - val_mae: 4.3476\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.6363 - mae: 3.9521 - val_loss: 33.3268 - val_mae: 4.3372\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.4636 - mae: 3.9505 - val_loss: 33.6440 - val_mae: 4.3654\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.4933 - mae: 3.9320 - val_loss: 33.6456 - val_mae: 4.3397\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b25bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d3ea1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error  :  33.64559101027034\n",
      "Mean Absolute Error :  4.339743543956794\n",
      "R2 Score            :  0.5411993005090976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "\n",
    "print(\"Mean Squared Error  : \",mean_squared_error(Y_test,y_pred))\n",
    "print(\"Mean Absolute Error : \",mean_absolute_error(Y_test,y_pred))\n",
    "print(\"R2 Score            : \",r2_score(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5e92273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=Y_test,y=y_pred)\n",
    "plt.title(\"Regression Line\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ae72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
